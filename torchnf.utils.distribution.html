<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torchnf.utils.distribution &mdash; torchnf 0.1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torchnf.utils.flow" href="torchnf.utils.flow.html" />
    <link rel="prev" title="torchnf.utils.decorators" href="torchnf.utils.decorators.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> torchnf
          </a>
              <div class="version">
                0.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="torchnf.html">torchnf</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="torchnf.abc.html">torchnf.abc</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.conditioners.html">torchnf.conditioners</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.layers.html">torchnf.layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.metrics.html">torchnf.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.model.html">torchnf.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.networks.html">torchnf.networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnf.transformers.html">torchnf.transformers</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="torchnf.utils.html">torchnf.utils</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.datasets.html">torchnf.utils.datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.decorators.html">torchnf.utils.decorators</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">torchnf.utils.distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.flow.html">torchnf.utils.flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.hooks.html">torchnf.utils.hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.mcmc.html">torchnf.utils.mcmc</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.optim.html">torchnf.utils.optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchnf.utils.tensor.html">torchnf.utils.tensor</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">torchnf</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="torchnf.html">torchnf</a> &raquo;</li>
          <li><a href="torchnf.utils.html">torchnf.utils</a> &raquo;</li>
      <li>torchnf.utils.distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/torchnf.utils.distribution.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-torchnf.utils.distribution">
<span id="torchnf-utils-distribution"></span><h1>torchnf.utils.distribution<a class="headerlink" href="#module-torchnf.utils.distribution" title="Permalink to this heading"></a></h1>
<p>A collection of utils for constructing objects based on distributions.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionDataModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchnf.utils.distribution.</span></span><span class="sig-name descname"><span class="pre">DistributionDataModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution" title="(in PyTorch v1.12)"><span class="pre">Distribution</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PositiveInt</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.10)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.10)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PositiveInt</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.10)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PositiveInt</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.10)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionDataModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionDataModule" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.LightningDataModule.html#pytorch_lightning.core.LightningDataModule" title="(in PyTorch Lightning v1.6.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code></a></p>
<p>Wraps a distribution in a DataModule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>distribution</strong> – The distribution serving as the prior for a Normalizing Flow</p></li>
<li><p><strong>batch_size</strong> – Size of each batch drawn from the distribution</p></li>
<li><p><strong>epoch_length</strong> – Number of batches constituting an ‘epoch’</p></li>
<li><p><strong>val/test/pred_batch_size</strong> – Batch sizes for the validation, test, predict steps, if they
should be different than the training <code class="docutils literal notranslate"><span class="pre">batch_size</span></code></p></li>
<li><p><strong>val/test/pred_epoch_length</strong> – Epoch lengths for validation, test, predict steps, if they
should be different than the training <code class="docutils literal notranslate"><span class="pre">epoch_length</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The epoch length has no significance whatsoever, since batches
are generated on demand and never recycled. However, since Pytorch
Lightning has lots of hooks which execute at the start and end of
an epoch, it can be convenient to define an epoch in terms of a
fixed number of batches.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionDataModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchnf.utils.distribution.IterableDistribution" title="torchnf.utils.distribution.IterableDistribution"><span class="pre">IterableDistribution</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionDataModule.predict_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionDataModule.predict_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterable version of the prior distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionDataModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchnf.utils.distribution.IterableDistribution" title="torchnf.utils.distribution.IterableDistribution"><span class="pre">IterableDistribution</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionDataModule.test_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionDataModule.test_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterable version of the prior distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionDataModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchnf.utils.distribution.IterableDistribution" title="torchnf.utils.distribution.IterableDistribution"><span class="pre">IterableDistribution</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionDataModule.train_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionDataModule.train_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterable version of the prior distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionDataModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchnf.utils.distribution.IterableDistribution" title="torchnf.utils.distribution.IterableDistribution"><span class="pre">IterableDistribution</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionDataModule.val_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionDataModule.val_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterable version of the prior distribution.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionLazyShape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchnf.utils.distribution.</span></span><span class="sig-name descname"><span class="pre">DistributionLazyShape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution" title="(in PyTorch v1.12)"><span class="pre">Distribution</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionLazyShape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionLazyShape" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Wraps a distribution to allow sampling various shapes.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.DistributionLazyShape.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#DistributionLazyShape.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.DistributionLazyShape.log_prob" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchnf.utils.distribution.IterableDistribution">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchnf.utils.distribution.</span></span><span class="sig-name descname"><span class="pre">IterableDistribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution" title="(in PyTorch v1.12)"><span class="pre">Distribution</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.10)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.10)"><span class="pre">Optional</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#IterableDistribution"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.IterableDistribution" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code></p>
<p>Wraps a distribution to allow sampling to be iterated over.</p>
<p>The motivation for this is that an instance of IterableDistribution
may be used as a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> in PyTorch Lightning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>distribution</strong> – The distribution, whose <code class="docutils literal notranslate"><span class="pre">sample</span></code> method will be called at
each iteration step</p></li>
<li><p><strong>batch_size</strong> – Batch size or shape returned by each iteration step</p></li>
<li><p><strong>length</strong> – Optionally specify a length for the generator, which is
interpreted by PyTorch Lightning as the number of steps in
a single ‘epoch’</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create a prior distribution using expand_independent</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uv_gauss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prior</span> <span class="o">=</span> <span class="n">expand_independent</span><span class="p">(</span><span class="n">uv_gauss</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prior</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([6, 6])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make an iterable prior with batch size 100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iprior</span> <span class="o">=</span> <span class="n">IterableDistribution</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">iprior</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 6, 6])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># iprior has the same attributes as prior</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iprior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.IterableDistribution.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#IterableDistribution.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.IterableDistribution.log_prob" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchnf.utils.distribution.IterableDistribution.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#IterableDistribution.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.IterableDistribution.sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchnf.utils.distribution.diagonal_gaussian">
<span class="sig-prename descclassname"><span class="pre">torchnf.utils.distribution.</span></span><span class="sig-name descname"><span class="pre">diagonal_gaussian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">event_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.normal.Normal" title="(in PyTorch v1.12)"><span class="pre">Normal</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#diagonal_gaussian"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.diagonal_gaussian" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Gaussian with null mean and unit diagonal covariance.</p>
<p>This is equivalent to calling <a class="reference internal" href="#torchnf.utils.distribution.expand_dist" title="torchnf.utils.distribution.expand_dist"><code class="xref py py-func docutils literal notranslate"><span class="pre">expand_dist()</span></code></a> with
<code class="docutils literal notranslate"><span class="pre">distribution=torch.distributions.Normal(0,</span> <span class="pre">1)</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchnf.utils.distribution.expand_dist">
<span class="sig-prename descclassname"><span class="pre">torchnf.utils.distribution.</span></span><span class="sig-name descname"><span class="pre">expand_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution" title="(in PyTorch v1.12)"><span class="pre">Distribution</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">event_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.10)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">PositiveInt</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.Size([])</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.independent.Independent" title="(in PyTorch v1.12)"><span class="pre">Independent</span></a></span></span><a class="reference internal" href="_modules/torchnf/utils/distribution.html#expand_dist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchnf.utils.distribution.expand_dist" title="Permalink to this definition"></a></dt>
<dd><p>Constructs a multivariate distribution with iid components.</p>
<p>The components of the resulting distribution are independent and
identically distributed according to the input distribution. The
resulting distribution has an event shape that is the concatenation
of <code class="docutils literal notranslate"><span class="pre">event_shape</span></code> with the shape(s) of the original distribution,
and a batch shape given by <code class="docutils literal notranslate"><span class="pre">batch_shape</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>distribution</strong> – The distribution of the iid components</p></li>
<li><p><strong>event_shape</strong> – The event shape of the resulting multivariate distribution,
not including the shape(s) of the original distribution</p></li>
<li><p><strong>batch_shape</strong> – The batch shape of the resulting distribution</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A multivariate distibution with independent components</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>This will create a multivariate Gaussian with diagonal covariance:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create a univariate Gaussian distribution</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uv_gauss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uv_gauss</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">uv_gauss</span><span class="o">.</span><span class="n">event_shape</span>
<span class="go">(torch.Size([]), torch.Size([]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uv_gauss</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create a multivariate Gaussian with diagonal covariance</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mv_gauss</span> <span class="o">=</span> <span class="n">expand_independent</span><span class="p">(</span><span class="n">uv_gauss</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mv_gauss</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">mv_gauss</span><span class="o">.</span><span class="n">event_shape</span>
<span class="go">(torch.Size([]), torch.Size([6, 6]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mv_gauss</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([6, 6])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># What happens when we compute the log-prob?</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uv_gauss</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">mv_gauss</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mv_gauss</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">mv_gauss</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchnf.utils.decorators.html" class="btn btn-neutral float-left" title="torchnf.utils.decorators" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="torchnf.utils.flow.html" class="btn btn-neutral float-right" title="torchnf.utils.flow" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Joe Marsh Rossney.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>